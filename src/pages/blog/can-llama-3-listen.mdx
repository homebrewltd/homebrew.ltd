---
title: Can llama learn to listen?
authorURL: https://twitter.com/homebrewltd
description: Homebrew research shares llama3-s, an open & ongoing research experiment to teach llama3 to listen.
tags: llama3, multimodal, llm, "ai training"
categories: research
ogImage: assets/images/og/llama-book.png
date: 2024-08-06
---

import { Callout } from 'nextra/components'
import BlogBackButton from '@/components/Blog/BackButton'
import BlogAuthors from '@/components/Blog/Authors'
import ResearchCTABlog from '@/components/Blog/ResearchCTABlog'

<BlogBackButton />

# Can llama learn to listen?

<BlogAuthors authors={["Alan Dao", "Rex Ha", "Bach Vu", "Phong Tran"]}/>

<Callout emoji="üìö">
We invite you to join [llama3-s](https://discord.gg/Q7dXd6b9Zb): an open and ongoing research experiment focused on teaching llama3 to listen. This blog post is a quick project update and an open call to join to us to build a speech adapter for open source models.
</Callout>

Inspired by the [Chameleon](https://arxiv.org/pdf/2405.09818) and [Llama Herd](https://arxiv.org/pdf/2407.21783) papers, llama3-s is an early-fusion, audio and text, multimodal model. We're conducting this research entirely in the open, with an open-source [codebase](https://github.com/homebrewltd/llama3-s), [open data](https://huggingface.co/datasets/homebrewltd/instruction-speech-v1.5) and [open weights](https://huggingface.co/homebrewltd/llama3-s-2024-07-19).

<br></br>

<iframe
    className="w-full aspect-video"
    src={`https://www.youtube.com/embed/1Nv831Bf-FY`}
    title="YouTube Video"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
></iframe>
*Gradio Demo (19 July 2024): the model ‚Äúlistens‚Äù to audio files and does text autocompletion*

Our initial approach uses a sound compressor to create discrete token representations of sound, which are then used to train the LLM. We hypothesize that these sound tokens can be continuously learned and that existing text-based models can find semantic connections between sound and text representations.

In this post, we share results from a [July 19th checkpoint](https://huggingface.co/homebrewltd/llama3-s-2024-07-19). We trained llama3 8b on synthetic audio, via a small training run that cost less than $600, yielding interesting results. But this is just the beginning, and we need your expertise and ideas to push this research further.

![Image Credit: When llama learns to listen by Cary & Kumar](./_assets/llama3s/llama-book.png)
*Image Credit: When llama learns to listen by Cary & Kumar*

## The Problem

[Cascaded systems](https://arxiv.org/abs/2301.10606) (currently popular in production) take user speech, transcribe it into text, and pass it into an LLM. They have been proven to be slow, inefficient, and [lossy](https://arxiv.org/pdf/2311.06753#page=10&zoom=100,88,489).

![Cascaded systems use 2 models. In addition to latency, emotion, tone, many audio features are lost.](./_assets/llama3s/cascaded-systems.png)
*Pictured: Cascaded systems use two separate models. In addition to latency, emotion, tone, many audio features are lost.*

![Multimodal models natively understand text, audio, image input.](./_assets/llama3s/multimodal-case.png)
*Pictured: Multimodal models natively understand text, audio, image input.*

Significant research in this space has been on training **multimodal models** that natively understand audio tokens and text tokens (see [Citations](#citations)). 

Usually, this entails pretraining encoders, adapters on large amounts of ASR, AST, synthetic, and human-generated datasets, making the endeavor quite data expensive and compute intensive. 

We wanted to see how far we‚Äôd get by just **continuously training llama3 on a small set of synthetic, high quality instruct QA pairs**. Our initial goal is a small, scaling law experiment, where any initial convergence of audio and text tokens could have interesting implications upon larger and more diverse datasets.

## Training: 19th July Checkpoint

This experiment ended up being quite achievable on a smaller budgeted run. Building on [existing research](#citations), we employed a unique combination of techniques. Hopefully our initial findings below contribute meaningful learnings.

**Data**: For training data, we collated open datasets, consisting of 6 million (15Bn) Q&A text pairs. This was then reduced to 2Mn instruction pairs (4B tokens) through deduping, and filtering for English ([FastText](https://github.com/facebookresearch/fastText)) and token length. We also used additional heuristics-based classifiers to filter out bad samples for additional quality. (See [limitations](#current-problems)).

![Fig 1. Data tokens length distribution](./_assets/llama3s/no-tokens.png)

Fig 1. Data tokens length distribution

**Audio generation:** We then used [WhisperSpeech](https://github.com/collabora/WhisperSpeech) text-to-speech model on the default voice tensor to generate an interleaving dataset of 2bn tokens (1Mn. wav files): 

- 80%: audio questions & text answers
- 20%: text questions & text answers (a subset of the 80% audio questions & text answers)

**Audio encoding**: The [Encodec](https://github.com/facebookresearch/encodec) VQ model was used to tokenized the audio files and create sound tokens, compressing each time step into 2D tensors, via 2 channels (24kHz audio file to 1.5 kps)  These audio tokens were then added to llama3‚Äôs existing token vocabulary. (See [limitations](#current-problems)).

You can find the datasets here: 

| Date | HF Checkpoint | Tokens |
| --- | --- | --- |
| üìÖ 2024-07-19 | üîó¬†https://huggingface.co/homebrew-research | üî¢ 1.35B |
| üìÖ 2024-07-18 | üîó¬†https://huggingface.co/datasets/homebrew-research/instruction-speech-v1.5 | üî¢ 800M |
| üìÖ 2024-06-30 | üîó¬†https://huggingface.co/datasets/homebrew-research/instruction-speech-v1 | üî¢ 450M |

**Training**: The instruct tuning was done with fsdp2 ([Torchtune](https://github.com/pytorch/torchtune)) mixed-precision, on a [llama3 8b](https://huggingface.co/meta-llama/Meta-Llama-3-8B) base model, with the final weights in bf16. We used the [AdamMini](https://arxiv.org/abs/2406.16793) optimizer, a global batchsize of 128 (mini-batches of 2-4), a 3e-4 learning rate, and a slightly longer warm up ratio. You can find the full steps to reproduce our training here on [Hugging Face](https://huggingface.co/homebrewltd/llama3-s-2024-07-19).

![19th July Checkpoint training hyperparams](./_assets/llama3s/training-params.png)

## Results: 19 July Run

This checkpoint was evaluated on held-out datasets not overlapping with training data. 

**Loss Evaluation**: After 2 billion tokens, the model stablised at a loss of ~1.0.

![Pictured: checkpoint 7-19 loss curve.](./_assets/llama3s/loss-graph.png)
*Pictured: checkpoint 7-19 loss curve.*

**Heuristics Evaluation**: We manually evaluated this checkpoint on 100 samples: 

A good answer from the checkpoint:
```yaml
# Question (in audio tokens):
Is the Paris Climate Change agreement going to help slow pollution?

# Expected Output (in text):
Yes, the Paris Climate Agreement is designed to help slow pollution and combat climate change on a global scale. The main goal of the agreement is to limit global warming to well below 2 degrees Celsius, preferably to 1.5 degrees Celsius, compared to pre-industrial levels. To achieve this, countries that have signed the agreement commit to reducing their greenhouse gas emissions...

# Model's Output (in text):
Yes, the Paris Climate Agreement is a global commitment to limit global warming to well below 2 degrees Celsius above pre-industrial levels and pursue efforts to limit it to 1.5 degrees Celsius. The agreement was signed by 195 countries and entered into force on November 4, 2016. 

The agreement sets out a framework for countries to reduce greenhouse gas emissions and adapt to the impacts of climate change. It also encourages countries to increase their ambition and to support developing countries in their efforts to reduce emissions. 

The Paris Agreement is a significant step towards addressing climate change, but it is not a guarantee of success. The agreement requires countries to submit Nationally Determined Contributions (NDCs) outlining their plans to reduce emissions and adapt to the impacts of climate change. The success of the agreement will depend on the ability of countries to meet their NDCs and to work together to address the global nature of climate change.```
```

Conversely, a bad answer from the checkpoint:

```yaml
# Question (in audio tokens):
what is the address to configure a mfm harddrive on a pc without a bios

# Expected Output (in text):
Configuring an MFM (Modified Frequency Modulation) hard drive on a PC without a BIOS (Basic Input/Output System) is a complex task that involves hardware jumpers and possibly some low-level software...

# Model's Output (in text):
The average computer memory (RAM) is typically around 4GB, but it can vary depending on the specific model and manufacturer.
```

Many of the bad answers were "sound similar" hallucinations, e.g. the model mishearing `Gasoline` for `Gaslow`, which is phonetically similar.

```yaml
# Question (in audio tokens):
Why is gasoline a liquid. 

# Expected Output (in text):
Gasoline is a liquid because it is a mixture of hydrogen and oxygen...

# Model's Output (in text):
Gaslow is a city in Scotland...
```

**Degradation Evaluation**: We evaluated whether the checkpoint retained original reasoning with 0 shot MMLU.

| Groups | Version | Original | New | Stderr |
| --- | --- | --- | --- | --- |
| mmlu | 1 | 0.6380 | 0.2478 | 0.0036 |
| - humanities | 1 | 0.5785 | 0.2504 | 0.0063 |
| - other | 1 | 0.7184 | 0.2530 | 0.0078 |
| - social sciences | 1 | 0.7413 | 0.2418 | 0.0077 |
| - stem | 1 | 0.5468 | 0.2448 | 0.0076 |

While this specific checkpoint demonstrated forgetting, it has since been addressed. It was a mistake in our tuning parameters, and a 30 July checkpoint with a similar sized training set yielded `0.6137`. 

**Results**: 

| Date | Checkpoint | Tokens | Steps | Loss | GPU hours | All in Cost |
| --- | --- | --- | --- | --- | --- | --- |
| üìÖ 2024-07-19 | üîó¬†https://huggingface.co/homebrewltd/llama3-s-2024-07-19 | üî¢ 1.35B | üîÑ 1195k | üìâ 1.0 | 112(14x8 H100 SXM) | $530 üòÇ |
| üìÖ 2024-07-01 | üîó¬†https://huggingface.co/homebrewltd/llama3-s-2024-07-08 | üî¢ 700M | üîÑ 1431k | üìâ 1.7-1.8 | 64(8x8 H100s SXM) | $280 |
| üìÖ 2024-06-23 | üîó¬†https://huggingface.co/homebrewltd/llama3-s-init | üî¢ 0M | üîÑ N/A | üìâ N/A | N/A | N/A |

<Callout>
We were pleasantly surprised at the initial results under such compute and data constraints. Though, it is too early to conjecture speech & text convergence. 

Minimally, we observed some transitive properties between an LLM‚Äôs existing latent text representations and newly learned audio tokens, through just instruct tuning llama3.
</Callout>

**Hardware**: for funsies, we ran our training workload cross various nodes, and measured the following.

![Same training workload across various nodes](./_assets/llama3s/hardware-comp.png)

## Current Problems

This 19th July checkpoint had limitations.

- Synthetic audio training data (not to mention our generated from a single voice tensor) have been shown to be less effective than ASR or crowdsourced audio.
- Audio compression encoders like Encodec underperform against semantic encoders, or custom encoders for speech.
- Pre-training and subsequent alignment may be strictly necessary.
- As we scale to bigger datasets or models, scaling laws may imply greater loss or degeneration.
- Possibly suboptimal warming-up ratio, learning rate, minor bugs.

## Next Steps

At the moment, we‚Äôre working on a new iteration with the following improvements: 

- Retraining phase 1 with llama3.1
- A more diverse synthetic speech set and encoder
- Various fixes to hyperparameters, improving degradation
- Training script optimizations and better benchmarking

**Have better ideas?** Join our [training in public](https://discord.gg/VSbRN3vwCD).

## Citations

- [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)
- [Chameleon: https://arxiv.org/abs/2405.09818](https://arxiv.org/abs/2304.09842)
- [AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs](https://arxiv.org/pdf/2311.06753)
- [AudioLM: a Language Modeling Approach to Audio Generation](https://arxiv.org/pdf/2209.03143)
- [Speech Recognition with Augmented Synthesized Speech](https://ieeexplore.ieee.org/document/9003990)
- [DASB: Discrete Audio and Speech Benchmark](https://arxiv.org/pdf/2406.14294)
- [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models](https://arxiv.org/pdf/2311.07919)

---

## Open Call

We‚Äôre calling on LLM researchers and audio experts to collaborate with us. As a nascent research team, Homebrew is our early days of ‚Äúthrow sh*t at the wall and see what sticks‚Äù. 

We openly discuss training recipes in Discord, argue it out, and often run the best ideas the next day.

Join the Discord fun:

- [#research](https://discord.gg/9NfUSyzp3y)¬†: general research & paper sharing
- [#llama3-s](https://discord.com/channels/1107178041848909847/1269847858379493442): daily ~~arguments~~ discussions
- [#research-livestream](https://discord.gg/BmA5DbxeEb): live training & lo-fi music üòÇ

We believe that collaborative, open research can accelerate progress in this exciting field. Whether you're an experienced researcher or an enthusiastic newcomer, your contribution could be valuable.

<Callout>
At [Homebrew Computer Company](https://homebrew.ltd), we like smaller, ‚Äúedge friendly‚Äù models that are privacy preserving and feasible to train on energy-efficient clusters. Read more about our [AI philosophy here](https://homebrew.ltd/about).
</Callout>

---

<ResearchCTABlog/>
